{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading-in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the required packages and modules. The last line imports the utility scoring function from the file that was published in Challenge's GitHub page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import TensorSpec as ts\n",
    "import pandas as pd\n",
    "from scipy.special import logit\n",
    "from matplotlib import pyplot as plt \n",
    "from tqdm import tqdm, trange\n",
    "from time import time\n",
    "from evaluate_sepsis_score import compute_prediction_utility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up some constants that govern how many CV folds we assume and which CV fold is used for testing, as well as number of batches for mini-batch model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time()\n",
    "dataDir = r\"E:\\AALTO\\Kevät2020\\STATISTICAL GENETICS  & PERS. MED\\Project\\data\"\n",
    "dtype = np.double\n",
    "reloadDataFlag = False\n",
    "cvFoldN, cvFold = 5, 0\n",
    "batchN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the data from the raw data files, do some truncation for the very outlying outcomes and save the preprocessed data to a single file that is much faster to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wspace = 0.5  # the amount of width reserved for space between subplots,\n",
    "              # expressed as a fraction of the average axis width\n",
    "hspace = 0.55  # the amount of height reserved for space between subplots,\n",
    "              # expressed as a fraction of the average axis height\n",
    "    \n",
    "if reloadDataFlag:\n",
    "  subDirList = [\"training_setA\", \"training_setB\"]\n",
    "  dataFrames, P = [], 0\n",
    "  for subDir in subDirList:\n",
    "    files = [f for f in os.listdir(os.path.join(dataDir,subDir)) if os.path.isfile(os.path.join(dataDir,subDir, f))]\n",
    "    for f in tqdm(files):\n",
    "      df = pd.read_csv(os.path.join(dataDir,subDir,f),sep=\"|\")\n",
    "      df[\"Id\"] = P\n",
    "      dataFrames.append(df)\n",
    "      P += 1\n",
    "  dataFrame = pd.concat(dataFrames)\n",
    "      \n",
    "  fig, axes = plt.subplots(nrows=5, ncols=7, figsize=(20,9))\n",
    "  for i,ax in enumerate(axes.flatten()):\n",
    "    q = np.nanquantile(dataFrame.iloc[:,i],[0,0.001,0.01,0.05,0.25,0.5,0.75,0.95,0.99,0.999,1])\n",
    "    dataFrame.iloc[:,i] = np.clip(dataFrame.iloc[:,i], q[1],q[-2])\n",
    "    ax.hist(dataFrame.iloc[:,i], bins=10, fc=(0, 1, 0, 1))\n",
    "    ax.set_title(str(i) + \" \" + dataFrame.columns[i])\n",
    "      \n",
    "      \n",
    "  plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
    "  plt.show()\n",
    "  \n",
    "  dataFrame.to_pickle(os.path.join(dataDir,\"all_raw_data.pkl\"))\n",
    "else:\n",
    "  dataFrame = pd.read_pickle(os.path.join(dataDir,\"all_raw_data.pkl\"))\n",
    "  P = np.unique(dataFrame[\"Id\"]).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We handle some discrepancies in the raw data files and fill in some missing covariates values (not outcomes).\n",
    "\n",
    "TODO:\n",
    "Here we could do our base comparison if wanted, to see how it affects the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dataFrame[\"Unit\"] = dataFrame[\"Unit1\"] + 2*dataFrame[\"Unit2\"] - 1 \n",
    "dataFrame.loc[np.isnan(dataFrame[\"Unit\"]),\"Unit\"] = 2\n",
    "dataFrame[\"HospAdmTime\"][np.isnan(dataFrame[\"HospAdmTime\"])] = np.nanmean(dataFrame[\"HospAdmTime\"])\n",
    "dataFrameList = [df for _, df in dataFrame.groupby(\"Id\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reprocess the SepsisLabel column into two columns: \"Deseased\" - whether this patient will get sepsis at some point and \"Time\" time past since the patient got sepsis (is negative if patient will get sepsis in future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 40336/40336 [00:52<00:00, 761.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for p in tqdm(range(P)):\n",
    "  df = dataFrameList[p]\n",
    "  ind = np.where(df[\"SepsisLabel\"]==1)[0]\n",
    "  if ind.size > 0:\n",
    "    indMin = ind[0]\n",
    "    df = df.assign(Time=np.maximum(df[\"ICULOS\"] - df.loc[df.index[indMin],\"ICULOS\"],-90), Deseased=1)\n",
    "  else:\n",
    "    df = df.assign(Time=np.nan, Deseased=0)\n",
    "  dataFrameList[p] = df\n",
    "dataFrame = pd.concat(dataFrameList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get numpy matrices of covariates and outcomes and perform some normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "YColumns = dataFrame.columns[:34]\n",
    "XColumns = [\"Time\", \"Deseased\", \"Age\", \"Gender\", \"Unit\", 'HospAdmTime', 'ICULOS', \"Id\"]\n",
    "timeColumnIndex = 6\n",
    "YAll = dataFrame[YColumns].values\n",
    "XAll = dataFrame[XColumns].values\n",
    "\n",
    "# Own addition\n",
    "YAll_before_norm = YAll\n",
    "XAll_before_norm = XAll\n",
    "\n",
    "YScale = np.array([np.nanmean(YAll,0),np.nanstd(YAll,0)])\n",
    "XScale = np.array([np.nanmean(XAll,0),np.nanstd(XAll,0)])\n",
    "XScale[:,-1] = [0,1]\n",
    "XScale[0,0] = 0\n",
    "YAll = (YAll - YScale[0]) / YScale[1]\n",
    "XAll = (XAll - XScale[0]) / XScale[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to training and validation partitions. \n",
    "\n",
    "TODO:\n",
    "I think it should be beneficial to check that all train,test and val had nice percentages of sespsis patientss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of patients who get sepsis:  2932\n",
      "The amount of patients who dont get sepsis:  37404\n",
      "All patients:  40336\n",
      "_______\n",
      "The amount of sepsis patients choosed in test set:  586\n",
      "The amount of sepsis patients choosed in train set 2346\n",
      "The amount of healthy patients choosed in test set 7481\n",
      "The amount of healthy patients choosed in train set 29923\n",
      "[27512. 27739. 33968. ... 39081. 29537. 35114.]\n",
      "[    0     1     2 ... 40332 40333 40335]\n"
     ]
    }
   ],
   "source": [
    "# Own code to test if better data division yields better results\n",
    "all_patients = np.arange(P)\n",
    "# Check which patients get sepsis at some point\n",
    "target_rows = XAll_before_norm[XAll_before_norm[:,1]==1]\n",
    "id_sepsis_patients = np.unique(target_rows[:,-1])\n",
    "\n",
    "# The training/testing ratio for sepsis patients \n",
    "test_amount_sep = np.rint(0.20*id_sepsis_patients.shape[0])\n",
    "sepsis_test = np.random.choice(id_sepsis_patients, test_amount_sep.astype(int),replace=False)\n",
    "sepsis_train = np.setdiff1d(id_sepsis_patients, sepsis_test)\n",
    "\n",
    "# The trainin/testing ratio for nor sepsis patients\n",
    "not_sepsis_patients = np.setdiff1d(all_patients, id_sepsis_patients)\n",
    "test_amount_notsep = np.rint(0.20*not_sepsis_patients.shape[0])\n",
    "not_sepsis_test = np.random.choice(not_sepsis_patients, test_amount_notsep.astype(int),replace=False)\n",
    "not_sepsis_train = np.setdiff1d(not_sepsis_patients, not_sepsis_test)\n",
    "\n",
    "print(\"The amount of patients who get sepsis: \",id_sepsis_patients.shape[0])\n",
    "print(\"The amount of patients who dont get sepsis: \", not_sepsis_patients.shape[0])\n",
    "print(\"All patients: \",all_patients.shape[0])\n",
    "print(\"_______\")\n",
    "print(\"The amount of sepsis patients choosed in test set: \",sepsis_test.shape[0])\n",
    "print(\"The amount of sepsis patients choosed in train set\",sepsis_train.shape[0])\n",
    "print(\"The amount of healthy patients choosed in test set\",not_sepsis_test.shape[0])\n",
    "print(\"The amount of healthy patients choosed in train set\", not_sepsis_train.shape[0])\n",
    "\n",
    "idVecTest = np.concatenate((not_sepsis_test,sepsis_test))\n",
    "idVecTrain = np.setdiff1d(all_patients, idVecTest)\n",
    "\n",
    "# The initial code continues below\n",
    "\n",
    "# Every fifth id to the testing [0]-> selects only the test indices to form list\n",
    "#idVecTest = np.where((np.arange(P) % cvFoldN) == cvFold)[0]\n",
    "# Everything else to training\n",
    "#idVecTrain = np.setdiff1d(np.arange(P), idVecTest)\n",
    "\n",
    "print(idVecTest)\n",
    "print(idVecTrain)\n",
    "\n",
    "# Make masks to get the training and testing sets\n",
    "indTrain = np.isin(XAll[:,-1], idVecTrain)\n",
    "indTest = np.isin(XAll[:,-1], idVecTest)\n",
    "XTrain, YTrain = XAll[indTrain], YAll[indTrain]\n",
    "XTest, YTest = XAll[indTest], YAll[indTest]\n",
    "\n",
    "# Take the id's out\n",
    "XTrain = XTrain[:,:-1]\n",
    "# Fill nan's with 0\n",
    "XTrain[np.isnan(XTrain)] = 0\n",
    "\n",
    "# Mask for the y nan values 0-> nan\n",
    "YoTrain = (~np.isnan(YTrain)).astype(dtype)\n",
    "# Fill nan's with 0\n",
    "YTrain[np.isnan(YTrain)] = 0\n",
    "N,J = YTrain.shape\n",
    "C = XTrain.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model set up and fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model by specifying its (log) likelihood function. It is really important to understand how the fromula from the project description file matches its implementation here! Pay attention to how missing Y values are handled. Check Wiki page on Normal distribution for the likelihood if it seems unfamiliar.\n",
    "\n",
    "This block is the primary part (and almost the only one) that you will have to change to get to models 2 and 3. Model 4 will require a wider range of adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment next line if you prefer TF eager execution mode\n",
    "@tf.function(input_signature=(ts([C,J],dtype),ts([J],dtype),ts([None,C],dtype),ts([None,J],dtype),ts([None,J],dtype)))\n",
    "def logLikelihood(B,sigma2, X,Y,Yo):\n",
    "    # Calculate Mean vector\n",
    "    Mu = tf.matmul(X, B)\n",
    "    print(Mu)\n",
    "    # Calculate Y-Mean vector -> Yo puts the nan- spots to zero\n",
    "    R = (Y - Mu) * Yo\n",
    "    # Calculate the log(sigma^2) -> Yo puts the nan- spots to zero\n",
    "    logDetD_vec = tf.reduce_sum(Yo * tf.math.log(sigma2), 1)\n",
    "    # Calculate the the square(Y-X)/sigma^2 \n",
    "    qF_vec = tf.reduce_sum(tf.square(R) / sigma2, 1)\n",
    "    # Put them all together to get the loglikelihood\n",
    "    constTerm_vec = -0.5*tf.reduce_sum(Yo, 1)*np.log(2*np.pi)\n",
    "    logLike_vec = -0.5*(logDetD_vec + qF_vec) + constTerm_vec\n",
    "    logLike = tf.reduce_sum(logLike_vec)\n",
    "    # Return value and vector? \n",
    "   \n",
    "    return logLike, logLike_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the set of model parameters that will be learned and an optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = tf.Variable(tf.zeros([C,J], dtype=dtype))\n",
    "sigma2_uncon = tf.Variable(tf.zeros([J], dtype=dtype))\n",
    "par_list = [B, sigma2_uncon]\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment next line if you prefer TF eager execution mode\n",
    "@tf.function(input_signature=(ts([None,C],dtype),ts([None,J],dtype),ts([None,J],dtype)))\n",
    "def optimizer_step(X,Y,Yo):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(par_list)\n",
    "    sigma2 = tf.math.softplus(sigma2_uncon)\n",
    "    loss = -logLikelihood(B,sigma2, X,Y,Yo)[0] * (N/tf.shape(X)[0])\n",
    "  grad_list = tape.gradient(loss, par_list)\n",
    "  optimizer.apply_gradients(zip(grad_list,par_list))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the optimization cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:00<?, ?it/s, batch=000, loss=00012181772.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43814608409900846 -0.41883934134864476 -0.4043592842858722 ... -0.027864070526150215 -0.27533871318070841 -0.15467157099093634]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:00<?, ?it/s, batch=001, loss=00012118699.028]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.75861335846927214 -0.63738723499861949 -0.42091201451531107 ... -0.54985902914159657 -0.41131488803227917 -0.38533786157428229]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:01<?, ?it/s, batch=002, loss=00012081565.883]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0169390686246573 -0.9577785535376494 -0.88678593543323991 ... -0.051990849900052351 -0.69630031711995777 -0.4833224628067293]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:01<?, ?it/s, batch=003, loss=00012048816.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1250272340382406 -0.88953556910555631 -0.84538088193067784 ... -0.9823147067908371 -0.7615412709164453 -0.58492252221693175]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:01<?, ?it/s, batch=004, loss=00012043595.750]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4594892811246742 -1.4081785561513049 -1.0661070563288462 ... -0.18361606251672111 -0.08099461256998336 -0.80585157371632987]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:01<?, ?it/s, batch=005, loss=00012032873.074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8122072018584363 -1.7930361070206617 -1.1603899773741078 ... -0.87034961715321824 -0.85117852231544378 -0.77449414296434649]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:01<?, ?it/s, batch=006, loss=00012003503.310]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3098643979693105 -2.1206938518133072 -1.9945801543759711 ... -1.3583262907645068 -1.2111936437542807 -1.0220230975982765]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:01<?, ?it/s, batch=007, loss=00012004888.733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2207237962400654 -1.9510102350203906 -1.9285341049187514 ... -1.4486434309768434 -1.3587389105702852 -1.2913105202653665]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                         | 0/100 [00:02<?, ?it/s, batch=008, loss=00011983730.894]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.7064781677556544 -2.6356212536065744 -2.5647643394574953 ... -1.4281833322275623 -1.3100884753124302 -1.1683746470142717]\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                | 1/100 [00:02<04:05,  2.47s/it, batch=009, loss=00011930712.369]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-26d7bba28150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0minBatchVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mbatchN\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m       \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minBatchVec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minBatchVec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYoTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minBatchVec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m       \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0mtran\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"%.3d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"%015.3f\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trange(100) as tran:\n",
    "  for epoch in tran:\n",
    "    ind = np.random.permutation(np.arange(YTrain.shape[0]))\n",
    "    for batch in range(batchN):\n",
    "      inBatchVec = ind%batchN==batch\n",
    "      X,Y,Yo = XTrain[inBatchVec],YTrain[inBatchVec],YoTrain[inBatchVec]\n",
    "      val = optimizer_step(X,Y,Yo)\n",
    "      tran.set_postfix(batch=\"%.3d\"%batch, loss=\"%015.3f\"%val.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes classifier based on generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the fitted generative model to calculate the likelihoods of the following hypotheses: A - the current patient will never ever get sepsis and B - the current patient will get sepsis at time $t_0$. Since we do not know the time $t_0$, we consider multiple opportunities. Namely, it seems reasonable that given that the current patient has been observed for $t$ hours now, we will consider that the onset of sepsis $t_0$ could happend in any hour between $t-t_{before}$ and $t+t_{after}$.\n",
    "\n",
    "For the sake of numerical performance efficiency the code below considers the $t_0$ being in range between hour $t_{before}$ and $T + t_{after}$, where $T$ is the total number of hours that the given patient will stay in the ICU (number of rows in patient's dataset). Obviously, this number is not availalby in advance and therefore can result in information leak from the future. In order to cope with this issue, the further sections adjust the predictions to be in line with how they are described in previous paragraph. When you work on your own implementations, please keep in mind this potential pitfall and avoid it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8067/8067 [07:13<00:00, 18.62it/s]\n"
     ]
    }
   ],
   "source": [
    "testN = idVecTest.shape[0]\n",
    "sigma2 = tf.math.softplus(sigma2_uncon)\n",
    "t0SpanBefore = 12\n",
    "t0SpanAfter = 48\n",
    "t0Span = t0SpanBefore + t0SpanAfter\n",
    "logLikelihood = logLikelihood\n",
    "resArrayList = []\n",
    "with tqdm(range(testN)) as progressBar:\n",
    "  for indTest in progressBar:\n",
    "    # The id of the targeted patient\n",
    "    idTest = idVecTest[indTest]\n",
    "    # Get the indexes where the boolean test is satisfied\n",
    "    indIdTest = np.where(XTest[:,-1] == idTest)[0]\n",
    "    # Select only the rows of targeted patient\n",
    "    XTestId,YTestId = XTest[indIdTest,:-1].copy(), YTest[indIdTest].copy()\n",
    "    # Get mask for nan. values in Y set\n",
    "    indNan = np.isnan(YTestId)\n",
    "    # Get reverse mask nan->0, useful data->1\n",
    "    YoTestId = (~indNan).astype(dtype)\n",
    "    # Change nan- values to 0\n",
    "    YTestId[indNan] = 0\n",
    "    # Reverse normalization for the time column\n",
    "    tOrig = XTestId[:,timeColumnIndex] * XScale[1,timeColumnIndex] + XScale[0,timeColumnIndex]\n",
    "    # The amount of rows the patient is in studied\n",
    "    N = XTestId.shape[0]\n",
    "    # Array full of nan's. For what purpose? \n",
    "    resArray = np.full([N,N+t0Span], np.nan)\n",
    "    Y,Yo = tf.constant(YTestId), tf.constant(YoTestId)\n",
    "    # Setting the X to 0 and 1 to get the probability for the current situation t0Ind\n",
    "    # And do time scaling (?)\n",
    "    for t0Ind in range(N+t0Span):\n",
    "      X = XTestId.copy()\n",
    "      if t0Ind==0:\n",
    "        # X[:,1]=deceased X[:,0]=time\n",
    "        X[:,1] = 0\n",
    "        X[:,0] = 0\n",
    "      else:\n",
    "        X[:,1] = 1\n",
    "        # Dont really understand this? \n",
    "        t0 = tOrig[0] - t0SpanBefore + t0Ind\n",
    "        X[:,0] = ((tOrig-t0)-XScale[0,0])/XScale[1,0]\n",
    "      X = tf.constant(X)\n",
    "      # Loglikelihood for the sepsis labels (?) \n",
    "      logLike = logLikelihood(B,sigma2, X,Y,Yo)[1]\n",
    "      # Get the probabilities to matrix as cumulative sum vector\n",
    "      resArray[:,t0Ind] = np.cumsum(logLike,0)\n",
    "    resArrayList.append(resArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the scoring rules of the Challenge, we compute the utility of trivial (inaction) predictions and optimal predictions. Those are used to get a score that shall be within the range $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/8067 [00:00<?, ?it/s]C:\\Users\\makke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  from ipykernel import kernelapp as app\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8067/8067 [00:16<00:00, 480.25it/s]\n"
     ]
    }
   ],
   "source": [
    "dt_early   = -12\n",
    "dt_optimal = -6\n",
    "dt_late    = 3\n",
    "best_utilities     = np.zeros(testN)\n",
    "inaction_utilities = np.zeros(testN)\n",
    "with tqdm(range(testN)) as progressBar:\n",
    "  for indTest in progressBar:\n",
    "    # Loop through the list of test patients\n",
    "    idTest = idVecTest[indTest]\n",
    "    # Get the indexes where the boolean test is satisfied\n",
    "    indIdTest = np.where(XTest[:,-1] == idTest)[0]\n",
    "    # Get the X and Y data for target patient \n",
    "    XTestId,YTestId = XTest[indIdTest,:-1].copy(), YTest[indIdTest].copy()\n",
    "    # Nan values to zero and make bool mask for the sepsis\n",
    "    sepLabelVec = ~np.isnan(XTestId[:,0]) * (XTestId[:,0]>=0.0)\n",
    "    N = XTestId.shape[0]\n",
    "    best_predictions     = np.zeros(N)\n",
    "    inaction_predictions = np.zeros(N)\n",
    "    action_predictions = np.ones(N)\n",
    "    if np.any(sepLabelVec):\n",
    "      # Optimal prediction time is the period of 6 hours after first 1 in sepsilabel  \n",
    "      t_sepsis = np.argmax(sepLabelVec) - dt_optimal\n",
    "      # The best prediction times for the patient\n",
    "      best_predictions[max(0, t_sepsis + dt_early) : min(t_sepsis + dt_late + 1, N)] = 1\n",
    "    # Save the best utilities for do-something and do-nothing (?)\n",
    "    best_utilities[indTest],_ = compute_prediction_utility(sepLabelVec, best_predictions)\n",
    "    inaction_utilities[indTest],_ = compute_prediction_utility(sepLabelVec, inaction_predictions)\n",
    "unnormalized_best_utility     = np.sum(best_utilities)\n",
    "unnormalized_inaction_utility = np.sum(inaction_utilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the obtained hypotheses' likelihoods into binary predictions and pass them to obtain the utilities. \n",
    "\n",
    "This section contains the largest space for testing and improvization since there is no clear optimal rule of how your model-based believes regarding whether and when the patient will get sepsis shall be transformed into final predictions. Current code implements a rule-based common sense version with some finetuning of couple parameters. But try to be creative and explore other opportunitiers (I guarantee that there are those that perform better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/8067 [00:00<?, ?it/s]C:\\Users\\makke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\makke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8067/8067 [00:21<00:00, 368.10it/s]\n"
     ]
    }
   ],
   "source": [
    "priorProb = 0.1\n",
    "dtAdvance = 6\n",
    "bfThreshhold = -0.4\n",
    "observed_utilities = np.zeros([testN])\n",
    "with tqdm(range(testN)) as progressBar:\n",
    "  for indTest in progressBar:\n",
    "    # Loop through the list of test patients \n",
    "    idTest = idVecTest[indTest]\n",
    "    # Select the patient's probabilities from the list\n",
    "    resArray = resArrayList[indTest]\n",
    "    # Select the covariate indexes for of current patient\n",
    "    indIdTest =  np.where(XTest[:,-1] == idTest)[0]\n",
    "    # The X and Y test values of the patient\n",
    "    XTestId,YTestId = XTest[indIdTest,:-1].copy(), YTest[indIdTest].copy()\n",
    "    # Nan values to zero and make bool mask for the sepsis\n",
    "    sepLabelVec = ~np.isnan(XTestId[:,0]) * (XTestId[:,0]>=0.0)\n",
    "    N = XTestId.shape[0]\n",
    "    isPositive = np.any(sepLabelVec)\n",
    "    A = np.tri(N, N+t0Span-1, t0Span-1) - np.tri(N, N+t0Span-1, -1)\n",
    "    A[A==0] = np.nan\n",
    "    A *= resArray[:,1:]-resArray[:,0,None]\n",
    "    B = A.copy()\n",
    "    B[:,:t0SpanBefore-1] = np.nan\n",
    "    neg = np.ones([N,1])\n",
    "    pos = ~np.isnan(B)\n",
    "    pos = pos / np.sum(pos,1)[:,None]\n",
    "    C = np.concatenate([np.zeros([N,1]),B],1)\n",
    "    C[np.isnan(C)] = -np.inf\n",
    "    logPrior = np.log(np.concatenate([(1-priorProb)*neg,priorProb*pos],1))\n",
    "    unnormPostProb = np.exp(C + logPrior)\n",
    "    normPostProb = unnormPostProb / np.sum(unnormPostProb,1)[:,None]\n",
    "    postProbVec = np.sum(normPostProb[:,1:]*np.tri(N,N+t0Span-1,dtAdvance+t0SpanBefore-1),1)\n",
    "    postBF = logit(postProbVec)\n",
    "    observed_predictions = postBF > bfThreshhold \n",
    "    observed_utilities[indTest],_ = compute_prediction_utility(sepLabelVec, observed_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the predictive score for the current CV fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictive utility score 0.023, elapsed time 889s\n"
     ]
    }
   ],
   "source": [
    "unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "print(\"\\nPredictive utility score %.3f, elapsed time %ds\"%(normalized_observed_utility, time() - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example with linear model ends up in somewhat $0.03$ utility score if we average over the 5 CV folds. This is very far behind the methods from the top of the leaderboard, but still better than the best trivial predictions. However, this is why this model is marked as number 1, so the other onces shall generally perform better. Also, remember that you are not requested to beat them, but rahter conducting a course project that is aimed to give you practical skills on multivariate generative modelling that would be applicable in a wide range of biomedical applications.\n",
    "\n",
    "Got in my tests with the initial code = 0.015 and 0.018 in two runs\n",
    "\n",
    "By dividing the data set differently (making sure 20% of sepsis labels actually are in testing) = 0.029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enjoy the exciting dataset and good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
